{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphRAG Workflow: Knowledge Graph-Enhanced Retrieval Augmented Generation\n",
    "\n",
    "This notebook implements a production-ready GraphRAG system that:\n",
    "1. Extracts entities and relationships from documents\n",
    "2. Builds a knowledge graph representation\n",
    "3. Performs graph-based retrieval for context\n",
    "4. Generates responses using graph-augmented context\n",
    "\n",
    "## Architecture Overview\n",
    "- **Document Processing**: Chunk and embed documents\n",
    "- **Entity Extraction**: LLM-based entity and relationship extraction\n",
    "- **Graph Construction**: NetworkX for graph storage and traversal\n",
    "- **Hybrid Retrieval**: Combines vector similarity and graph traversal\n",
    "- **Augmented Generation**: Context-aware response generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain langchain-openai langchain-community \n",
    "!pip install -q chromadb networkx pyvis\n",
    "!pip install -q sentence-transformers tiktoken\n",
    "!pip install -q python-dotenv tqdm\n",
    "!pip install -q pandas numpy matplotlib seaborn\n",
    "!pip install -q spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional, Tuple, Set\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set up environment\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Data Models and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Entity:\n",
    "    \"\"\"Represents an entity in the knowledge graph\"\"\"\n",
    "    id: str\n",
    "    name: str\n",
    "    type: str\n",
    "    description: Optional[str] = None\n",
    "    properties: Dict[str, Any] = field(default_factory=dict)\n",
    "    embedding: Optional[np.ndarray] = None\n",
    "    source_chunks: List[str] = field(default_factory=list)\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.id)\n",
    "\n",
    "@dataclass\n",
    "class Relationship:\n",
    "    \"\"\"Represents a relationship between entities\"\"\"\n",
    "    source_id: str\n",
    "    target_id: str\n",
    "    type: str\n",
    "    description: Optional[str] = None\n",
    "    properties: Dict[str, Any] = field(default_factory=dict)\n",
    "    confidence: float = 1.0\n",
    "    source_chunks: List[str] = field(default_factory=list)\n",
    "\n",
    "@dataclass\n",
    "class GraphConfig:\n",
    "    \"\"\"Configuration for GraphRAG system\"\"\"\n",
    "    chunk_size: int = 1000\n",
    "    chunk_overlap: int = 200\n",
    "    max_entities_per_chunk: int = 10\n",
    "    max_relationships_per_chunk: int = 15\n",
    "    embedding_model: str = \"text-embedding-3-small\"\n",
    "    llm_model: str = \"gpt-4o-mini\"\n",
    "    temperature: float = 0.1\n",
    "    graph_traversal_depth: int = 2\n",
    "    similarity_threshold: float = 0.7\n",
    "    top_k_retrieval: int = 5\n",
    "    \n",
    "config = GraphConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Processing and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    \"\"\"Handles document chunking and preprocessing\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GraphConfig):\n",
    "        self.config = config\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=config.chunk_size,\n",
    "            chunk_overlap=config.chunk_overlap,\n",
    "            length_function=self._tiktoken_len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "        self.encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "        \n",
    "    def _tiktoken_len(self, text: str) -> int:\n",
    "        \"\"\"Calculate token length using tiktoken\"\"\"\n",
    "        return len(self.encoding.encode(text))\n",
    "    \n",
    "    def process_documents(self, documents: List[str]) -> List[Document]:\n",
    "        \"\"\"Process documents into chunks with metadata\"\"\"\n",
    "        all_chunks = []\n",
    "        \n",
    "        for doc_idx, doc_text in enumerate(documents):\n",
    "            # Create chunks\n",
    "            chunks = self.text_splitter.split_text(doc_text)\n",
    "            \n",
    "            # Add metadata\n",
    "            for chunk_idx, chunk in enumerate(chunks):\n",
    "                chunk_id = hashlib.md5(f\"{doc_idx}_{chunk_idx}_{chunk[:50]}\".encode()).hexdigest()[:12]\n",
    "                doc = Document(\n",
    "                    page_content=chunk,\n",
    "                    metadata={\n",
    "                        \"doc_id\": doc_idx,\n",
    "                        \"chunk_id\": chunk_id,\n",
    "                        \"chunk_index\": chunk_idx,\n",
    "                        \"token_count\": self._tiktoken_len(chunk),\n",
    "                        \"timestamp\": datetime.now().isoformat()\n",
    "                    }\n",
    "                )\n",
    "                all_chunks.append(doc)\n",
    "        \n",
    "        logger.info(f\"Processed {len(documents)} documents into {len(all_chunks)} chunks\")\n",
    "        return all_chunks\n",
    "\n",
    "# Initialize processor\n",
    "doc_processor = DocumentProcessor(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entity and Relationship Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityExtractor:\n",
    "    \"\"\"Extracts entities and relationships using LLM\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GraphConfig):\n",
    "        self.config = config\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=config.llm_model,\n",
    "            temperature=config.temperature\n",
    "        )\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # Entity extraction prompt\n",
    "        self.entity_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        Extract entities and relationships from the following text.\n",
    "        Focus on key concepts, people, organizations, locations, and their relationships.\n",
    "        \n",
    "        Text: {text}\n",
    "        \n",
    "        Return a JSON with the following structure:\n",
    "        {{\n",
    "            \"entities\": [\n",
    "                {{\n",
    "                    \"name\": \"entity name\",\n",
    "                    \"type\": \"PERSON/ORGANIZATION/CONCEPT/LOCATION/OTHER\",\n",
    "                    \"description\": \"brief description\",\n",
    "                    \"properties\": {{\"key\": \"value\"}}\n",
    "                }}\n",
    "            ],\n",
    "            \"relationships\": [\n",
    "                {{\n",
    "                    \"source\": \"source entity name\",\n",
    "                    \"target\": \"target entity name\",\n",
    "                    \"type\": \"relationship type\",\n",
    "                    \"description\": \"relationship description\"\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "        \n",
    "        Limit to {max_entities} entities and {max_relationships} relationships.\n",
    "        \"\"\")\n",
    "        \n",
    "    def extract_from_chunk(self, chunk: Document) -> Tuple[List[Entity], List[Relationship]]:\n",
    "        \"\"\"Extract entities and relationships from a single chunk\"\"\"\n",
    "        try:\n",
    "            # LLM extraction\n",
    "            response = self.llm.invoke(self.entity_prompt.format_messages(\n",
    "                text=chunk.page_content,\n",
    "                max_entities=self.config.max_entities_per_chunk,\n",
    "                max_relationships=self.config.max_relationships_per_chunk\n",
    "            ))\n",
    "            \n",
    "            # Parse JSON response\n",
    "            extraction = json.loads(response.content)\n",
    "            \n",
    "            # Create Entity objects\n",
    "            entities = []\n",
    "            for ent_data in extraction.get(\"entities\", []):\n",
    "                entity = Entity(\n",
    "                    id=hashlib.md5(ent_data[\"name\"].lower().encode()).hexdigest()[:12],\n",
    "                    name=ent_data[\"name\"],\n",
    "                    type=ent_data.get(\"type\", \"OTHER\"),\n",
    "                    description=ent_data.get(\"description\"),\n",
    "                    properties=ent_data.get(\"properties\", {}),\n",
    "                    source_chunks=[chunk.metadata[\"chunk_id\"]]\n",
    "                )\n",
    "                entities.append(entity)\n",
    "            \n",
    "            # Create Relationship objects\n",
    "            relationships = []\n",
    "            for rel_data in extraction.get(\"relationships\", []):\n",
    "                source_id = hashlib.md5(rel_data[\"source\"].lower().encode()).hexdigest()[:12]\n",
    "                target_id = hashlib.md5(rel_data[\"target\"].lower().encode()).hexdigest()[:12]\n",
    "                \n",
    "                relationship = Relationship(\n",
    "                    source_id=source_id,\n",
    "                    target_id=target_id,\n",
    "                    type=rel_data[\"type\"],\n",
    "                    description=rel_data.get(\"description\"),\n",
    "                    source_chunks=[chunk.metadata[\"chunk_id\"]]\n",
    "                )\n",
    "                relationships.append(relationship)\n",
    "            \n",
    "            return entities, relationships\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting from chunk: {e}\")\n",
    "            return [], []\n",
    "    \n",
    "    def extract_from_documents(self, chunks: List[Document]) -> Tuple[Dict[str, Entity], List[Relationship]]:\n",
    "        \"\"\"Extract entities and relationships from all chunks\"\"\"\n",
    "        all_entities = {}\n",
    "        all_relationships = []\n",
    "        \n",
    "        for chunk in tqdm(chunks, desc=\"Extracting entities\"):\n",
    "            entities, relationships = self.extract_from_chunk(chunk)\n",
    "            \n",
    "            # Merge entities\n",
    "            for entity in entities:\n",
    "                if entity.id in all_entities:\n",
    "                    # Merge source chunks\n",
    "                    all_entities[entity.id].source_chunks.extend(entity.source_chunks)\n",
    "                    # Update properties\n",
    "                    all_entities[entity.id].properties.update(entity.properties)\n",
    "                else:\n",
    "                    all_entities[entity.id] = entity\n",
    "            \n",
    "            all_relationships.extend(relationships)\n",
    "        \n",
    "        logger.info(f\"Extracted {len(all_entities)} unique entities and {len(all_relationships)} relationships\")\n",
    "        return all_entities, all_relationships\n",
    "\n",
    "# Initialize extractor\n",
    "entity_extractor = EntityExtractor(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Knowledge Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeGraph:\n",
    "    \"\"\"Manages the knowledge graph structure and operations\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GraphConfig):\n",
    "        self.config = config\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.embeddings_model = OpenAIEmbeddings(model=config.embedding_model)\n",
    "        self.entities = {}\n",
    "        self.entity_embeddings = {}\n",
    "        \n",
    "    def build_graph(self, entities: Dict[str, Entity], relationships: List[Relationship]):\n",
    "        \"\"\"Build the knowledge graph from entities and relationships\"\"\"\n",
    "        self.entities = entities\n",
    "        \n",
    "        # Add nodes\n",
    "        for entity_id, entity in entities.items():\n",
    "            self.graph.add_node(\n",
    "                entity_id,\n",
    "                name=entity.name,\n",
    "                type=entity.type,\n",
    "                description=entity.description,\n",
    "                properties=entity.properties,\n",
    "                source_chunks=entity.source_chunks\n",
    "            )\n",
    "        \n",
    "        # Add edges\n",
    "        for rel in relationships:\n",
    "            if rel.source_id in entities and rel.target_id in entities:\n",
    "                self.graph.add_edge(\n",
    "                    rel.source_id,\n",
    "                    rel.target_id,\n",
    "                    type=rel.type,\n",
    "                    description=rel.description,\n",
    "                    properties=rel.properties,\n",
    "                    confidence=rel.confidence,\n",
    "                    source_chunks=rel.source_chunks\n",
    "                )\n",
    "        \n",
    "        # Generate embeddings for entities\n",
    "        self._generate_entity_embeddings()\n",
    "        \n",
    "        logger.info(f\"Graph built with {self.graph.number_of_nodes()} nodes and {self.graph.number_of_edges()} edges\")\n",
    "    \n",
    "    def _generate_entity_embeddings(self):\n",
    "        \"\"\"Generate embeddings for all entities\"\"\"\n",
    "        entity_texts = []\n",
    "        entity_ids = []\n",
    "        \n",
    "        for entity_id, entity in self.entities.items():\n",
    "            text = f\"{entity.name} ({entity.type}): {entity.description or 'No description'}\"\n",
    "            entity_texts.append(text)\n",
    "            entity_ids.append(entity_id)\n",
    "        \n",
    "        if entity_texts:\n",
    "            embeddings = self.embeddings_model.embed_documents(entity_texts)\n",
    "            for entity_id, embedding in zip(entity_ids, embeddings):\n",
    "                self.entity_embeddings[entity_id] = np.array(embedding)\n",
    "                self.entities[entity_id].embedding = np.array(embedding)\n",
    "    \n",
    "    def get_subgraph(self, entity_ids: List[str], depth: int = 2) -> nx.DiGraph:\n",
    "        \"\"\"Get subgraph around given entities up to specified depth\"\"\"\n",
    "        nodes_to_include = set(entity_ids)\n",
    "        \n",
    "        for _ in range(depth):\n",
    "            new_nodes = set()\n",
    "            for node in nodes_to_include:\n",
    "                if node in self.graph:\n",
    "                    # Add predecessors and successors\n",
    "                    new_nodes.update(self.graph.predecessors(node))\n",
    "                    new_nodes.update(self.graph.successors(node))\n",
    "            nodes_to_include.update(new_nodes)\n",
    "        \n",
    "        return self.graph.subgraph(nodes_to_include)\n",
    "    \n",
    "    def find_similar_entities(self, query_embedding: np.ndarray, top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Find entities similar to query based on embeddings\"\"\"\n",
    "        similarities = []\n",
    "        \n",
    "        for entity_id, entity_embedding in self.entity_embeddings.items():\n",
    "            similarity = np.dot(query_embedding, entity_embedding) / (\n",
    "                np.linalg.norm(query_embedding) * np.linalg.norm(entity_embedding)\n",
    "            )\n",
    "            similarities.append((entity_id, similarity))\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "    \n",
    "    def get_graph_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistics about the graph\"\"\"\n",
    "        stats = {\n",
    "            \"num_nodes\": self.graph.number_of_nodes(),\n",
    "            \"num_edges\": self.graph.number_of_edges(),\n",
    "            \"density\": nx.density(self.graph),\n",
    "            \"num_connected_components\": nx.number_weakly_connected_components(self.graph),\n",
    "            \"avg_degree\": np.mean([d for n, d in self.graph.degree()]),\n",
    "            \"entity_type_distribution\": {}\n",
    "        }\n",
    "        \n",
    "        # Entity type distribution\n",
    "        for entity_id, entity in self.entities.items():\n",
    "            entity_type = entity.type\n",
    "            stats[\"entity_type_distribution\"][entity_type] = \\\n",
    "                stats[\"entity_type_distribution\"].get(entity_type, 0) + 1\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Initialize knowledge graph\n",
    "knowledge_graph = KnowledgeGraph(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hybrid Retrieval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridRetriever:\n",
    "    \"\"\"Combines vector search and graph traversal for retrieval\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GraphConfig, knowledge_graph: KnowledgeGraph):\n",
    "        self.config = config\n",
    "        self.knowledge_graph = knowledge_graph\n",
    "        self.embeddings_model = OpenAIEmbeddings(model=config.embedding_model)\n",
    "        self.vector_store = None\n",
    "        self.chunks_map = {}\n",
    "        \n",
    "    def index_chunks(self, chunks: List[Document]):\n",
    "        \"\"\"Index document chunks for vector search\"\"\"\n",
    "        # Create vector store\n",
    "        self.vector_store = Chroma.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=self.embeddings_model,\n",
    "            collection_name=\"graphrag_chunks\"\n",
    "        )\n",
    "        \n",
    "        # Store chunks map\n",
    "        for chunk in chunks:\n",
    "            self.chunks_map[chunk.metadata[\"chunk_id\"]] = chunk\n",
    "        \n",
    "        logger.info(f\"Indexed {len(chunks)} chunks in vector store\")\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Perform hybrid retrieval combining vector search and graph traversal\"\"\"\n",
    "        # Generate query embedding\n",
    "        query_embedding = np.array(self.embeddings_model.embed_query(query))\n",
    "        \n",
    "        # 1. Vector search for relevant chunks\n",
    "        vector_results = self.vector_store.similarity_search_with_score(query, k=top_k)\n",
    "        \n",
    "        # 2. Find similar entities in graph\n",
    "        similar_entities = self.knowledge_graph.find_similar_entities(\n",
    "            query_embedding, top_k=top_k\n",
    "        )\n",
    "        \n",
    "        # 3. Get subgraph around similar entities\n",
    "        entity_ids = [eid for eid, _ in similar_entities if _ > self.config.similarity_threshold]\n",
    "        subgraph = self.knowledge_graph.get_subgraph(\n",
    "            entity_ids, \n",
    "            depth=self.config.graph_traversal_depth\n",
    "        )\n",
    "        \n",
    "        # 4. Collect all relevant chunks\n",
    "        relevant_chunks = set()\n",
    "        \n",
    "        # From vector search\n",
    "        for doc, score in vector_results:\n",
    "            relevant_chunks.add(doc.metadata[\"chunk_id\"])\n",
    "        \n",
    "        # From graph entities\n",
    "        for node_id in subgraph.nodes():\n",
    "            entity = self.knowledge_graph.entities.get(node_id)\n",
    "            if entity:\n",
    "                relevant_chunks.update(entity.source_chunks)\n",
    "        \n",
    "        # 5. Prepare context\n",
    "        context = {\n",
    "            \"query\": query,\n",
    "            \"vector_results\": [\n",
    "                {\n",
    "                    \"content\": doc.page_content,\n",
    "                    \"metadata\": doc.metadata,\n",
    "                    \"score\": float(score)\n",
    "                }\n",
    "                for doc, score in vector_results\n",
    "            ],\n",
    "            \"graph_entities\": [\n",
    "                {\n",
    "                    \"id\": eid,\n",
    "                    \"name\": self.knowledge_graph.entities[eid].name,\n",
    "                    \"type\": self.knowledge_graph.entities[eid].type,\n",
    "                    \"description\": self.knowledge_graph.entities[eid].description,\n",
    "                    \"similarity\": float(sim)\n",
    "                }\n",
    "                for eid, sim in similar_entities\n",
    "            ],\n",
    "            \"graph_relationships\": [\n",
    "                {\n",
    "                    \"source\": self.knowledge_graph.entities[u].name,\n",
    "                    \"target\": self.knowledge_graph.entities[v].name,\n",
    "                    \"type\": data.get(\"type\"),\n",
    "                    \"description\": data.get(\"description\")\n",
    "                }\n",
    "                for u, v, data in subgraph.edges(data=True)\n",
    "            ],\n",
    "            \"relevant_chunk_ids\": list(relevant_chunks),\n",
    "            \"subgraph_stats\": {\n",
    "                \"num_nodes\": subgraph.number_of_nodes(),\n",
    "                \"num_edges\": subgraph.number_of_edges()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return context\n",
    "\n",
    "# Initialize retriever\n",
    "retriever = HybridRetriever(config, knowledge_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Graph-Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphRAGGenerator:\n",
    "    \"\"\"Generates responses using graph-augmented context\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GraphConfig):\n",
    "        self.config = config\n",
    "        self.llm = ChatOpenAI(\n",
    "            model=config.llm_model,\n",
    "            temperature=config.temperature\n",
    "        )\n",
    "        \n",
    "        self.generation_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        You are an AI assistant with access to a knowledge graph and document chunks.\n",
    "        Use the provided context to answer the question comprehensively.\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Document Context:\n",
    "        {document_context}\n",
    "        \n",
    "        Relevant Entities:\n",
    "        {entities_context}\n",
    "        \n",
    "        Entity Relationships:\n",
    "        {relationships_context}\n",
    "        \n",
    "        Instructions:\n",
    "        1. Synthesize information from both documents and the knowledge graph\n",
    "        2. Cite specific entities and relationships when relevant\n",
    "        3. Provide a comprehensive answer that leverages the graph structure\n",
    "        4. If information is missing or unclear, state that explicitly\n",
    "        \n",
    "        Answer:\n",
    "        \"\"\")\n",
    "    \n",
    "    def generate(self, retrieval_context: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate response using retrieved context\"\"\"\n",
    "        \n",
    "        # Prepare context strings\n",
    "        document_context = \"\\n\\n\".join([\n",
    "            f\"[Chunk {i+1} - Score: {r['score']:.3f}]\\n{r['content'][:500]}...\"\n",
    "            for i, r in enumerate(retrieval_context[\"vector_results\"][:3])\n",
    "        ])\n",
    "        \n",
    "        entities_context = \"\\n\".join([\n",
    "            f\"- {e['name']} ({e['type']}): {e['description'] or 'No description'} [Similarity: {e['similarity']:.3f}]\"\n",
    "            for e in retrieval_context[\"graph_entities\"][:5]\n",
    "        ])\n",
    "        \n",
    "        relationships_context = \"\\n\".join([\n",
    "            f\"- {r['source']} --[{r['type']}]--> {r['target']}: {r['description'] or ''}\"\n",
    "            for r in retrieval_context[\"graph_relationships\"][:10]\n",
    "        ])\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.llm.invoke(self.generation_prompt.format_messages(\n",
    "            query=retrieval_context[\"query\"],\n",
    "            document_context=document_context,\n",
    "            entities_context=entities_context,\n",
    "            relationships_context=relationships_context\n",
    "        ))\n",
    "        \n",
    "        return {\n",
    "            \"query\": retrieval_context[\"query\"],\n",
    "            \"response\": response.content,\n",
    "            \"metadata\": {\n",
    "                \"num_chunks_used\": len(retrieval_context[\"vector_results\"]),\n",
    "                \"num_entities_used\": len(retrieval_context[\"graph_entities\"]),\n",
    "                \"num_relationships_used\": len(retrieval_context[\"graph_relationships\"]),\n",
    "                \"subgraph_size\": retrieval_context[\"subgraph_stats\"],\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Initialize generator\n",
    "generator = GraphRAGGenerator(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complete GraphRAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphRAGPipeline:\n",
    "    \"\"\"End-to-end GraphRAG pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, config: GraphConfig):\n",
    "        self.config = config\n",
    "        self.doc_processor = DocumentProcessor(config)\n",
    "        self.entity_extractor = EntityExtractor(config)\n",
    "        self.knowledge_graph = KnowledgeGraph(config)\n",
    "        self.retriever = HybridRetriever(config, self.knowledge_graph)\n",
    "        self.generator = GraphRAGGenerator(config)\n",
    "        self.chunks = []\n",
    "        self.is_initialized = False\n",
    "    \n",
    "    def index_documents(self, documents: List[str]):\n",
    "        \"\"\"Index documents and build knowledge graph\"\"\"\n",
    "        logger.info(\"Starting document indexing...\")\n",
    "        \n",
    "        # Process documents into chunks\n",
    "        self.chunks = self.doc_processor.process_documents(documents)\n",
    "        \n",
    "        # Extract entities and relationships\n",
    "        entities, relationships = self.entity_extractor.extract_from_documents(self.chunks)\n",
    "        \n",
    "        # Build knowledge graph\n",
    "        self.knowledge_graph.build_graph(entities, relationships)\n",
    "        \n",
    "        # Index chunks for vector search\n",
    "        self.retriever.index_chunks(self.chunks)\n",
    "        \n",
    "        self.is_initialized = True\n",
    "        logger.info(\"Document indexing complete\")\n",
    "        \n",
    "        return self.knowledge_graph.get_graph_statistics()\n",
    "    \n",
    "    def query(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Query the GraphRAG system\"\"\"\n",
    "        if not self.is_initialized:\n",
    "            raise ValueError(\"Pipeline not initialized. Please index documents first.\")\n",
    "        \n",
    "        logger.info(f\"Processing query: {question}\")\n",
    "        \n",
    "        # Retrieve context\n",
    "        retrieval_context = self.retriever.retrieve(question)\n",
    "        \n",
    "        # Generate response\n",
    "        result = self.generator.generate(retrieval_context)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def visualize_graph(self, output_file: str = \"knowledge_graph.html\"):\n",
    "        \"\"\"Visualize the knowledge graph\"\"\"\n",
    "        if not self.is_initialized:\n",
    "            raise ValueError(\"Pipeline not initialized. Please index documents first.\")\n",
    "        \n",
    "        net = Network(height=\"750px\", width=\"100%\", bgcolor=\"#222222\", font_color=\"white\")\n",
    "        \n",
    "        # Add nodes\n",
    "        for node_id, node_data in self.knowledge_graph.graph.nodes(data=True):\n",
    "            entity = self.knowledge_graph.entities[node_id]\n",
    "            color = {\n",
    "                \"PERSON\": \"#ff6b6b\",\n",
    "                \"ORGANIZATION\": \"#4ecdc4\",\n",
    "                \"LOCATION\": \"#45b7d1\",\n",
    "                \"CONCEPT\": \"#96ceb4\",\n",
    "                \"OTHER\": \"#dfe6e9\"\n",
    "            }.get(entity.type, \"#dfe6e9\")\n",
    "            \n",
    "            net.add_node(\n",
    "                node_id,\n",
    "                label=entity.name,\n",
    "                title=f\"{entity.name}\\n{entity.description or 'No description'}\",\n",
    "                color=color,\n",
    "                size=20 + len(self.knowledge_graph.graph.edges(node_id)) * 3\n",
    "            )\n",
    "        \n",
    "        # Add edges\n",
    "        for source, target, edge_data in self.knowledge_graph.graph.edges(data=True):\n",
    "            net.add_edge(\n",
    "                source, \n",
    "                target,\n",
    "                title=f\"{edge_data.get('type', 'RELATED')}\\n{edge_data.get('description', '')}\",\n",
    "                label=edge_data.get('type', 'RELATED')\n",
    "            )\n",
    "        \n",
    "        net.save_graph(output_file)\n",
    "        logger.info(f\"Graph visualization saved to {output_file}\")\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = GraphRAGPipeline(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Example Usage and Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents for demonstration\n",
    "sample_documents = [\n",
    "    \"\"\"\n",
    "    Artificial Intelligence (AI) has revolutionized many industries, particularly healthcare and finance.\n",
    "    In healthcare, companies like DeepMind have developed AI systems for protein folding prediction,\n",
    "    while IBM Watson has been used for cancer diagnosis and treatment recommendations.\n",
    "    Stanford University's AI lab has been pioneering research in computer vision and natural language processing.\n",
    "    Dr. Andrew Ng, a prominent AI researcher from Stanford, founded Coursera and has been instrumental\n",
    "    in democratizing AI education. The field has seen rapid advancement with transformer architectures\n",
    "    developed by Google Research, which led to models like BERT and GPT.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    Machine learning applications in finance include fraud detection, algorithmic trading, and credit scoring.\n",
    "    JPMorgan Chase has invested heavily in AI for risk assessment and portfolio optimization.\n",
    "    Their COiN platform uses natural language processing to analyze legal documents.\n",
    "    Goldman Sachs employs machine learning for market prediction and automated trading strategies.\n",
    "    The collaboration between financial institutions and tech companies like Google and Microsoft\n",
    "    has accelerated the adoption of AI in finance. Regulatory bodies like the SEC are developing\n",
    "    frameworks for AI governance in financial markets.\n",
    "    \"\"\",\n",
    "    \n",
    "    \"\"\"\n",
    "    The ethical implications of AI have become a central concern for researchers and policymakers.\n",
    "    The Partnership on AI, founded by major tech companies including Google, Facebook, Amazon, and Microsoft,\n",
    "    works on establishing best practices for AI development. Dr. Timnit Gebru's research on bias in AI\n",
    "    has highlighted the importance of diversity in AI development teams. The European Union has proposed\n",
    "    comprehensive AI regulations focusing on transparency and accountability. Universities like MIT and\n",
    "    Oxford have established AI ethics research centers to study the societal impact of AI technologies.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Index documents\n",
    "print(\"Indexing documents...\")\n",
    "stats = pipeline.index_documents(sample_documents)\n",
    "print(\"\\nGraph Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example queries\n",
    "queries = [\n",
    "    \"What companies are working on AI in healthcare?\",\n",
    "    \"How is AI being used in finance?\",\n",
    "    \"What are the connections between universities and AI research?\",\n",
    "    \"Who are the key researchers mentioned and their contributions?\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    result = pipeline.query(query)\n",
    "    results.append(result)\n",
    "    \n",
    "    print(f\"Response: {result['response']}\")\n",
    "    print(f\"\\nMetadata:\")\n",
    "    for key, value in result['metadata'].items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the knowledge graph\n",
    "pipeline.visualize_graph(\"knowledge_graph.html\")\n",
    "print(\"Knowledge graph visualization saved to knowledge_graph.html\")\n",
    "\n",
    "# Display graph statistics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Entity type distribution\n",
    "stats = pipeline.knowledge_graph.get_graph_statistics()\n",
    "entity_types = list(stats['entity_type_distribution'].keys())\n",
    "entity_counts = list(stats['entity_type_distribution'].values())\n",
    "\n",
    "axes[0, 0].bar(entity_types, entity_counts, color='skyblue')\n",
    "axes[0, 0].set_title('Entity Type Distribution')\n",
    "axes[0, 0].set_xlabel('Entity Type')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Degree distribution\n",
    "degrees = [d for n, d in pipeline.knowledge_graph.graph.degree()]\n",
    "axes[0, 1].hist(degrees, bins=20, color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].set_title('Node Degree Distribution')\n",
    "axes[0, 1].set_xlabel('Degree')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Query performance metrics\n",
    "query_lengths = [len(q) for q in queries]\n",
    "response_lengths = [len(r['response']) for r in results]\n",
    "\n",
    "axes[1, 0].scatter(query_lengths, response_lengths, s=100, alpha=0.6, color='coral')\n",
    "axes[1, 0].set_title('Query vs Response Length')\n",
    "axes[1, 0].set_xlabel('Query Length (characters)')\n",
    "axes[1, 0].set_ylabel('Response Length (characters)')\n",
    "\n",
    "# Retrieval metrics\n",
    "metrics = ['Chunks', 'Entities', 'Relationships']\n",
    "avg_metrics = [\n",
    "    np.mean([r['metadata']['num_chunks_used'] for r in results]),\n",
    "    np.mean([r['metadata']['num_entities_used'] for r in results]),\n",
    "    np.mean([r['metadata']['num_relationships_used'] for r in results])\n",
    "]\n",
    "\n",
    "axes[1, 1].bar(metrics, avg_metrics, color=['#ff9999', '#66b3ff', '#99ff99'])\n",
    "axes[1, 1].set_title('Average Retrieval Metrics per Query')\n",
    "axes[1, 1].set_ylabel('Average Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced Features and Production Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphRAGMonitor:\n",
    "    \"\"\"Monitoring and observability for GraphRAG system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            \"queries_processed\": 0,\n",
    "            \"avg_response_time\": [],\n",
    "            \"avg_entities_per_query\": [],\n",
    "            \"avg_chunks_per_query\": [],\n",
    "            \"error_count\": 0,\n",
    "            \"query_history\": []\n",
    "        }\n",
    "    \n",
    "    def log_query(self, query: str, result: Dict[str, Any], response_time: float):\n",
    "        \"\"\"Log query metrics\"\"\"\n",
    "        self.metrics[\"queries_processed\"] += 1\n",
    "        self.metrics[\"avg_response_time\"].append(response_time)\n",
    "        self.metrics[\"avg_entities_per_query\"].append(result['metadata']['num_entities_used'])\n",
    "        self.metrics[\"avg_chunks_per_query\"].append(result['metadata']['num_chunks_used'])\n",
    "        \n",
    "        self.metrics[\"query_history\"].append({\n",
    "            \"query\": query,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"response_time\": response_time,\n",
    "            \"metadata\": result['metadata']\n",
    "        })\n",
    "    \n",
    "    def log_error(self, error: Exception, context: Dict[str, Any]):\n",
    "        \"\"\"Log errors\"\"\"\n",
    "        self.metrics[\"error_count\"] += 1\n",
    "        logger.error(f\"GraphRAG Error: {error}, Context: {context}\")\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get monitoring summary\"\"\"\n",
    "        return {\n",
    "            \"total_queries\": self.metrics[\"queries_processed\"],\n",
    "            \"avg_response_time\": np.mean(self.metrics[\"avg_response_time\"]) if self.metrics[\"avg_response_time\"] else 0,\n",
    "            \"avg_entities_per_query\": np.mean(self.metrics[\"avg_entities_per_query\"]) if self.metrics[\"avg_entities_per_query\"] else 0,\n",
    "            \"avg_chunks_per_query\": np.mean(self.metrics[\"avg_chunks_per_query\"]) if self.metrics[\"avg_chunks_per_query\"] else 0,\n",
    "            \"error_rate\": self.metrics[\"error_count\"] / max(self.metrics[\"queries_processed\"], 1),\n",
    "            \"recent_queries\": self.metrics[\"query_history\"][-5:]\n",
    "        }\n",
    "\n",
    "# Initialize monitor\n",
    "monitor = GraphRAGMonitor()\n",
    "\n",
    "# Example monitoring\n",
    "import time\n",
    "\n",
    "for query in queries[:2]:\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        result = pipeline.query(query)\n",
    "        response_time = time.time() - start_time\n",
    "        monitor.log_query(query, result, response_time)\n",
    "    except Exception as e:\n",
    "        monitor.log_error(e, {\"query\": query})\n",
    "\n",
    "print(\"\\nMonitoring Summary:\")\n",
    "for key, value in monitor.get_summary().items():\n",
    "    if key != \"recent_queries\":\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphRAGCache:\n",
    "    \"\"\"Caching layer for GraphRAG queries\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int = 100):\n",
    "        self.cache = {}\n",
    "        self.max_size = max_size\n",
    "        self.hit_count = 0\n",
    "        self.miss_count = 0\n",
    "    \n",
    "    def get_cache_key(self, query: str) -> str:\n",
    "        \"\"\"Generate cache key for query\"\"\"\n",
    "        return hashlib.md5(query.lower().strip().encode()).hexdigest()\n",
    "    \n",
    "    def get(self, query: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Get cached result\"\"\"\n",
    "        key = self.get_cache_key(query)\n",
    "        if key in self.cache:\n",
    "            self.hit_count += 1\n",
    "            logger.info(f\"Cache hit for query: {query[:50]}...\")\n",
    "            return self.cache[key]\n",
    "        self.miss_count += 1\n",
    "        return None\n",
    "    \n",
    "    def set(self, query: str, result: Dict[str, Any]):\n",
    "        \"\"\"Cache result\"\"\"\n",
    "        if len(self.cache) >= self.max_size:\n",
    "            # Remove oldest entry (simple FIFO)\n",
    "            oldest_key = next(iter(self.cache))\n",
    "            del self.cache[oldest_key]\n",
    "        \n",
    "        key = self.get_cache_key(query)\n",
    "        self.cache[key] = result\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get cache statistics\"\"\"\n",
    "        total_requests = self.hit_count + self.miss_count\n",
    "        hit_rate = self.hit_count / max(total_requests, 1)\n",
    "        \n",
    "        return {\n",
    "            \"cache_size\": len(self.cache),\n",
    "            \"max_size\": self.max_size,\n",
    "            \"hit_count\": self.hit_count,\n",
    "            \"miss_count\": self.miss_count,\n",
    "            \"hit_rate\": hit_rate,\n",
    "            \"total_requests\": total_requests\n",
    "        }\n",
    "\n",
    "# Initialize cache\n",
    "cache = GraphRAGCache(max_size=50)\n",
    "\n",
    "# Test caching\n",
    "test_query = \"What companies are working on AI in healthcare?\"\n",
    "\n",
    "# First query (miss)\n",
    "cached_result = cache.get(test_query)\n",
    "if not cached_result:\n",
    "    result = pipeline.query(test_query)\n",
    "    cache.set(test_query, result)\n",
    "    print(\"Query executed and cached\")\n",
    "\n",
    "# Second query (hit)\n",
    "cached_result = cache.get(test_query)\n",
    "if cached_result:\n",
    "    print(\"Retrieved from cache\")\n",
    "\n",
    "print(\"\\nCache Statistics:\")\n",
    "for key, value in cache.get_stats().items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphRAGEvaluator:\n",
    "    \"\"\"Evaluation metrics for GraphRAG system\"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline: GraphRAGPipeline):\n",
    "        self.pipeline = pipeline\n",
    "        self.embeddings_model = OpenAIEmbeddings(model=config.embedding_model)\n",
    "    \n",
    "    def evaluate_retrieval_relevance(self, query: str, retrieved_chunks: List[str]) -> float:\n",
    "        \"\"\"Evaluate relevance of retrieved chunks\"\"\"\n",
    "        query_embedding = np.array(self.embeddings_model.embed_query(query))\n",
    "        chunk_embeddings = [np.array(e) for e in self.embeddings_model.embed_documents(retrieved_chunks)]\n",
    "        \n",
    "        similarities = [\n",
    "            np.dot(query_embedding, chunk_emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(chunk_emb))\n",
    "            for chunk_emb in chunk_embeddings\n",
    "        ]\n",
    "        \n",
    "        return np.mean(similarities)\n",
    "    \n",
    "    def evaluate_graph_coverage(self, query_result: Dict[str, Any]) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate how well the graph was utilized\"\"\"\n",
    "        total_entities = len(self.pipeline.knowledge_graph.entities)\n",
    "        total_relationships = self.pipeline.knowledge_graph.graph.number_of_edges()\n",
    "        \n",
    "        used_entities = query_result['metadata']['num_entities_used']\n",
    "        used_relationships = query_result['metadata']['num_relationships_used']\n",
    "        \n",
    "        return {\n",
    "            \"entity_coverage\": used_entities / max(total_entities, 1),\n",
    "            \"relationship_coverage\": used_relationships / max(total_relationships, 1),\n",
    "            \"graph_utilization_score\": (used_entities + used_relationships) / max(total_entities + total_relationships, 1)\n",
    "        }\n",
    "    \n",
    "    def evaluate_response_quality(self, query: str, response: str) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate response quality metrics\"\"\"\n",
    "        # Simple heuristics - in production, use more sophisticated metrics\n",
    "        \n",
    "        return {\n",
    "            \"response_length\": len(response),\n",
    "            \"word_count\": len(response.split()),\n",
    "            \"sentence_count\": len([s for s in response.split('.') if s.strip()]),\n",
    "            \"query_terms_in_response\": sum(1 for term in query.lower().split() if term in response.lower()) / len(query.split())\n",
    "        }\n",
    "    \n",
    "    def run_evaluation(self, test_queries: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Run comprehensive evaluation\"\"\"\n",
    "        evaluation_results = []\n",
    "        \n",
    "        for query in test_queries:\n",
    "            result = self.pipeline.query(query)\n",
    "            retrieval_context = self.pipeline.retriever.retrieve(query)\n",
    "            \n",
    "            # Get retrieved chunks\n",
    "            retrieved_chunks = [r['content'] for r in retrieval_context['vector_results']]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            relevance_score = self.evaluate_retrieval_relevance(query, retrieved_chunks)\n",
    "            coverage_metrics = self.evaluate_graph_coverage(result)\n",
    "            quality_metrics = self.evaluate_response_quality(query, result['response'])\n",
    "            \n",
    "            evaluation_results.append({\n",
    "                \"query\": query[:50] + \"...\",\n",
    "                \"retrieval_relevance\": relevance_score,\n",
    "                **coverage_metrics,\n",
    "                **quality_metrics\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(evaluation_results)\n",
    "\n",
    "# Run evaluation\n",
    "evaluator = GraphRAGEvaluator(pipeline)\n",
    "eval_df = evaluator.run_evaluation(queries)\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(eval_df.to_string())\n",
    "\n",
    "print(\"\\nAverage Metrics:\")\n",
    "numeric_columns = eval_df.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_columns:\n",
    "    print(f\"  {col}: {eval_df[col].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Production Deployment Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production configuration template\n",
    "production_config = {\n",
    "    \"deployment\": {\n",
    "        \"environment\": \"production\",\n",
    "        \"api_endpoint\": \"https://api.graphrag.example.com\",\n",
    "        \"max_concurrent_requests\": 100,\n",
    "        \"request_timeout\": 30,\n",
    "        \"retry_policy\": {\n",
    "            \"max_retries\": 3,\n",
    "            \"backoff_factor\": 2,\n",
    "            \"retry_on_status\": [500, 502, 503, 504]\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"scaling\": {\n",
    "        \"auto_scaling\": True,\n",
    "        \"min_instances\": 2,\n",
    "        \"max_instances\": 10,\n",
    "        \"target_cpu_utilization\": 70,\n",
    "        \"scale_up_threshold\": 80,\n",
    "        \"scale_down_threshold\": 30\n",
    "    },\n",
    "    \n",
    "    \"storage\": {\n",
    "        \"vector_store\": {\n",
    "            \"type\": \"chroma\",\n",
    "            \"persistence_path\": \"/data/chroma\",\n",
    "            \"collection_name\": \"graphrag_production\"\n",
    "        },\n",
    "        \"graph_store\": {\n",
    "            \"type\": \"neo4j\",\n",
    "            \"uri\": \"bolt://neo4j:7687\",\n",
    "            \"database\": \"graphrag\"\n",
    "        },\n",
    "        \"cache\": {\n",
    "            \"type\": \"redis\",\n",
    "            \"host\": \"redis.example.com\",\n",
    "            \"port\": 6379,\n",
    "            \"ttl\": 3600\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"monitoring\": {\n",
    "        \"metrics_backend\": \"prometheus\",\n",
    "        \"logging_level\": \"INFO\",\n",
    "        \"log_aggregation\": \"elasticsearch\",\n",
    "        \"alerting\": {\n",
    "            \"enabled\": True,\n",
    "            \"channels\": [\"email\", \"slack\"],\n",
    "            \"thresholds\": {\n",
    "                \"error_rate\": 0.05,\n",
    "                \"response_time_p95\": 5000,\n",
    "                \"cpu_utilization\": 85\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"security\": {\n",
    "        \"authentication\": \"oauth2\",\n",
    "        \"rate_limiting\": {\n",
    "            \"enabled\": True,\n",
    "            \"requests_per_minute\": 60,\n",
    "            \"requests_per_hour\": 1000\n",
    "        },\n",
    "        \"encryption\": {\n",
    "            \"at_rest\": True,\n",
    "            \"in_transit\": True,\n",
    "            \"algorithm\": \"AES-256-GCM\"\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    \"optimization\": {\n",
    "        \"batch_processing\": True,\n",
    "        \"batch_size\": 32,\n",
    "        \"async_processing\": True,\n",
    "        \"connection_pooling\": True,\n",
    "        \"query_optimization\": {\n",
    "            \"cache_embeddings\": True,\n",
    "            \"precompute_graph_traversals\": True,\n",
    "            \"index_hot_paths\": True\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Production Configuration Template:\")\n",
    "print(json.dumps(production_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates a complete GraphRAG implementation with:\n",
    "\n",
    "### Core Features\n",
    "- **Document Processing**: Intelligent chunking with token counting\n",
    "- **Entity Extraction**: LLM-based entity and relationship extraction\n",
    "- **Knowledge Graph**: NetworkX-based graph construction and management\n",
    "- **Hybrid Retrieval**: Combined vector search and graph traversal\n",
    "- **Augmented Generation**: Context-aware response generation\n",
    "\n",
    "### Production Features\n",
    "- **Monitoring**: Comprehensive metrics and observability\n",
    "- **Caching**: Query result caching for performance\n",
    "- **Evaluation**: Multiple metrics for system assessment\n",
    "- **Error Handling**: Robust error handling throughout\n",
    "- **Configuration**: Flexible configuration management\n",
    "\n",
    "### Next Steps\n",
    "1. **Scale Testing**: Test with larger document collections\n",
    "2. **Graph Database**: Migrate to Neo4j for production scale\n",
    "3. **Advanced Extraction**: Implement more sophisticated entity extraction\n",
    "4. **Query Understanding**: Add query intent classification\n",
    "5. **Multi-hop Reasoning**: Implement complex graph traversal strategies\n",
    "6. **API Development**: Build REST/GraphQL API layer\n",
    "7. **UI Development**: Create visualization and query interface\n",
    "\n",
    "The system is designed to be modular and extensible, allowing for easy customization and enhancement based on specific use cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}